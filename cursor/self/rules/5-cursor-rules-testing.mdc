---
description: "Testing and verification methods for rules and code quality. Apply these guidelines when validating rule effectiveness, creating tests, establishing quality metrics, or implementing verification mechanisms. These standards ensure rules produce consistent, measurable improvements and can be objectively evaluated."
globs: ""
alwaysApply: false
---

# Testing Cursor Rules

## 1. Key Testing Principles (P0)

- **Reproducibility** - Tests must produce consistent results
- **Clarity** - Test results should be easy to understand
- **Coverage** - Tests should cover all aspects of the rule
- **Objectivity** - Tests should measure objective criteria
- **Traceability** - Test failures should be traceable to specific issues

## 2. Test Types (P0)

### 2.1. Format Validation Tests

- Verify YAML frontmatter structure
- Check markdown syntax validity
- Confirm heading structure follows guidelines
- Validate link references

### 2.2. Applicability Tests

- Verify that glob patterns correctly match intended files
- Ensure rule exclusions work as expected
- Test edge cases for glob pattern matching

### 2.3. Rule Effectiveness Tests

- Measure how well the rule addresses its intended purpose
- Track improvement in code quality metrics
- Gather user feedback on rule clarity and usefulness

## 3. Testing Methods (P1)

### 3.1. Manual Testing

1. Follow checklist of requirements
2. Review for clarity and completeness
3. Verify that examples reflect best practices
4. Check for consistent terminology

### 3.2. Automated Testing

- Run markdownlint for style checking
- Validate YAML with schema checks
- Test glob patterns against sample codebases
- Check for broken links and references

## 4. Testing Tools (P1)

| Tool | Purpose | Priority |
|------|---------|----------|
| markdownlint | Validate markdown syntax | P0 |
| yamllint | Check YAML frontmatter | P0 |
| link-checker | Verify links are valid | P1 |
| glob-tester | Test glob pattern matching | P1 |
| spell-checker | Find spelling errors | P2 |

## 5. Testing Process (P1)

1. **Pre-commit testing**
   - Run automated format validation
   - Verify frontmatter structure
   - Check for broken links

2. **Review testing**
   - Peer review by another team member
   - Verify rule clarity and usefulness
   - Test against sample code

3. **Integration testing**
   - Apply rule to actual codebases
   - Verify rule interactions with other rules
   - Collect metrics on effectiveness

4. **Self-verification testing**
   - Implement automatic self-checks
   - Include validation questions within rules
   - Create verification checklists

## 6. LLM Self-Verification (P0)

Every rule document MUST include basic verification questions:

1. **Basic verification questions**:
   - Does this rule apply to the current file?
   - Is the rule being followed correctly?
   - What specific aspects need improvement?

2. **Advanced verification mechanisms**:
   ```markdown
   ## Verification Questions
   
   When applying this rule, ask yourself:
   1. Does the current code follow the pattern described?
   2. Are there any exceptions that should be made?
   3. Is the implementation consistent with related rules?
   4. What metrics would indicate successful application?
   ```

3. **Required placement**:
   - Self-verification section MUST be included near the end of each rule document
   - Questions MUST be specific to the rule's domain
   - ALWAYS include both general and rule-specific verification questions

## 7. Context Budget Optimization (P0)

- Rules should occupy â‰¤20% of total context window
- Testing documentation should be concise
- Verification steps should be clear and direct
- Examples should demonstrate concepts efficiently

## 8. Quality Metrics (P2)

- **Clarity score** - Based on readability metrics
- **Precision** - How accurately the rule targets issues
- **Recall** - How completely the rule covers issues
- **Action rate** - How often the rule leads to changes
- **False positive rate** - How often the rule falsely flags issues

## 9. Rule Refinement (P2)

- Collect feedback from actual rule application
- Track metrics on rule effectiveness
- Periodically review and update based on new insights
- Maintain versioning for significant changes

## 10. Workflow Testing (P1)

Test the entire development workflow:

1. Assess user preferences and priorities
2. Collect input on all technical aspects
3. Prioritize based on project needs
4. Implement in priority order
5. Validate with LLM testing
6. Collect metrics and feedback
7. Refine rules based on results

## 11. Success Criteria Validation (P1)

Validate that rules meet their defined success criteria:

- Verify metrics collection is working
- Review feedback mechanisms
- Assess impact on code quality
- Measure developer productivity effects
- Track long-term maintenance costs
- MUST ensure all required aspects are covered (see @6-cursor-rules-coverage.mdc)

## 12. Release Checklist (P1)

Before releasing a new rule, MUST verify:

1. [ ] Format validation passes
2. [ ] Applicability tests confirm correct targeting
3. [ ] All examples are valid and follow best practices
4. [ ] Related rules are PROPERLY cross-referenced with `@` prefix (e.g., MUST refer to @1-cursor-rules-guide.mdc)
5. [ ] Rule has been peer-reviewed
6. [ ] Rule is written in English only

## 13. Language Considerations (P0)

- All test documentation must be in English
- Error messages should be clear and actionable
- Use consistent terminology across testing documentation
- Define technical terms when they're first used 

## 14. Consistency Testing (P0)

1. CRITICAL: Prior to finalizing any rule, perform these consistency tests:

   - **Duplication Test**: 
     - Scan all rule files for similar content
     - Flag any duplicate explanations, examples, or guidelines
     - MUST consolidate duplicate content to a single authoritative source
   
   - **Contradiction Test**:
     - Compare rule with all related rules
     - Verify no contradictory guidance exists
     - Resolve any conflicting instructions
   
   - **Reference Test**:
     - Ensure all references to other rules are valid
     - Verify referenced rules contain the expected content
     - Check that reference targets haven't been moved or renamed
   
   - **Context Usage Test**:
     - Measure token consumption of rules
     - Identify opportunities to reference rather than duplicate
     - Optimize for maximum information within context limits

2. REQUIRED: Document the results of consistency tests before approval 